# @package _global_ # This might be a convention used by the framework to signal that the following configuration is intended to be applied globally within the project.

model: #Defines the main model structure.
  _target_: sam2.modeling.sam2_base.SAM2Base # This indicates that the model is based on the SAM2Base class, likely from a library named sam2.
  
  image_encoder: # Image Encoder: Takes an image as input and extracts features.
    _target_: sam2.modeling.backbones.image_encoder.ImageEncoder
    scalp: 1 # Likely related to resolution of the image encoder.
    trunk: # Represents the backbone of the image encoder, often a CNN
      _target_: sam2.modeling.backbones.hieradet.Hiera # Encoder backbone is hiera (Hierarchical Vision Transformer), arxiv link is mentioned in source code
      embed_dim: 112 # Dimensionality of the feature vectors produced by encoder
      num_heads: 2 # No.of attention heads in a transformer-based layer in encoder
    
    neck: # The middle part of the architecture, between encoder and decoder
      _target_: sam2.modeling.backbones.image_encoder.FpnNeck # Feature Pyramid Network (FPN) for combining features from different levels of encoder backbone
      position_encoding: # Encode spatial information into the features, useful for detection and segmentation tasks
        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine # Common method for positional encoding using sinusoidal functions
        num_pos_feats: 256
        normalize: true
        scale: null
        temperature: 10000
      d_model: 256 # Dimensionality of the features processed by the FPN.
      backbone_channel_list: [896, 448, 224, 112] # No.of channels in different levels of FPN backbone.
      fpn_top_down_levels: [2, 3] # Defines the levels of the FPN that directly use features from the backbone.
      fpn_interp_model: nearest # Interpolation method used in the FPN for upsampling features.

  memory_attention: # Defines a memory attention mechanism, often used to maintain a history of past observations or interactions.
    _target_: sam2.modeling.memory_attention.MemoryAttention
    d_model: 256 # Dimensionality of the features used in the memory attention.
    pos_enc_at_input: true # Indicates that positional encoding is applied at the input of the memory attention.
    layer: # The specific layer within the memory attention mechanism.
      _target_: sam2.modeling.memory_attention.MemoryAttentionLayer
      activation: relu
      dim_feedforward: 2048
      dropout: 0.1
      pos_enc_at_attn: false

      self_attention: # Self-attention mechanism, which attends to different parts of the input sequence or feature map.
        _target_: sam2.modeling.sam.transformer.RoPEAttention # Type of Self-Attention called RoPE (Rotary Position Embedding) to encode positional information
        rope_theta: 10000.0
        feat_sizes: [32, 32]
        embedding_dim: 256
        num_heads: 1
        downsample_rate: 1
        dropout: 0.1
      d_model: 256
      pos_enc_at_cross_attn_keys: true
      pos_enc_at_cross_attn_queries: false

      cross_attention: # Cross-Attention mechanism, which allows the model to attend to information from different parts of the input or from other sources.
        _target_: sam2.modeling.sam.transformer.RoPEAttention
        rope_theta: 10000.0
        feat_sizes: [32, 32]
        rope_k_repeat: True
        embedding_dim: 256
        num_heads: 1
        downsample_rate: 1
        dropout: 0.1
        kv_in_dim: 64
    num_layers: 4

  memory_encoder: # Defines the memory encoder, which processes and encodes the information from the memory attention module.
      _target_: sam2.modeling.memory_encoder.MemoryEncoder
      out_dim: 64 # Output dimension from the memory encoder.
      position_encoding: # Positional encoding used in the memory encoder.
        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
        num_pos_feats: 64
        normalize: true
        scale: null
        temperature: 10000
      mask_downsampler: # Module for downsampling the mask representations.
        _target_: sam2.modeling.memory_encoder.MaskDownSampler
        kernel_size: 3
        stride: 2
        padding: 1
      fuser: # Module for fusing features from different levels of the memory encoder.
        _target_: sam2.modeling.memory_encoder.Fuser
        layer:
          _target_: sam2.modeling.memory_encoder.CXBlock
          dim: 256
          kernel_size: 7
          padding: 3
          layer_scale_init_value: 1e-6
          use_dwconv: True  # depth-wise convs
        num_layers: 2

  num_maskmem: 7 # Likely no.of memory slots or cells used for storing mask representations.
  image_size: 1024 # Expected input image size for the model
  
  # apply scaled sigmoid on mask logits for memory encoder, and directly feed input mask as output mask
  sigmoid_scale_for_mem_enc: 20.0
  sigmoid_bias_for_mem_enc: -10.0
  use_mask_input_as_output_without_sam: true
  # Memory
  directly_add_no_mem_embed: true
  # use high-resolution feature map in the SAM mask decoder
  use_high_res_features_in_sam: true
  # output 3 masks on the first click on initial conditioning frames
  multimask_output_in_sam: true
  # SAM heads
  iou_prediction_use_sigmoid: True
  # cross-attend to object pointers from other frames (based on SAM output tokens) in the encoder
  use_obj_ptrs_in_encoder: true
  add_tpos_enc_to_obj_ptrs: false
  only_obj_ptrs_in_the_past_for_eval: true
  # object occlusion prediction
  pred_obj_scores: true
  pred_obj_scores_mlp: true
  fixed_no_obj_ptr: true
  # multimask tracking settings
  multimask_output_for_tracking: true
  use_multimask_token_for_obj_ptr: true
  multimask_min_pt_num: 0
  multimask_max_pt_num: 1
  use_mlp_for_obj_ptr_proj: true
  # Compilation flag
  compile_image_encoder: False